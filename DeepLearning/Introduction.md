# 학습이란?

## 학습(Learning)
* Input(데이터)가 들어오게 되면 모델을 통해 예측을 하고 예측된 값을 정답과 비교하며 오차를 피드백하며 학습을 진행.
 * cheating(모델이 학습하던 데이터셋을 테스트하면 예측이 잘되는 현상)이 생기게 됨을 방지하기 위해서 데이터셋을 분할함.
* 모델을 통해 학습한 내용을 반복적으로 피드백하면 모델의 예측이 높아지게 됨.

## 지도학습(Supervised Learning), 비지도학습(Unsupervied Learning), 반지도학습(Semi-supervised Learning)
* 지도학습
    Dataset = Data + Label
* 비지도학습
    Dateset = Data
* 반지도학습
    지도학습 + 비지도학습

# 선형회귀(Linear regression)

## 선형회귀란?
* 데이터를 일반화하는 선을 찾는다.
* 그 선을 이용해 새로운 데이터를 예측한다.
* 직석의 방정식 y = Wx + b에서 W는 가중치를 의미하고, b는 편향을 의미한다.

## 회귀 직선
* H(x) = Wx + b
* error = H(x) - y (H(x)는 예측, y는 정답)

## 비용 함수(Cost function)
* error제곱(음수를 방지하기 위해)의 합.
* 선형 회귀는 cost를 가장 작게하는 직선.

# 신경망(Neural Networks)
* 딥러닝 알고리즘의 핵심.
* 하나 이상의 은닉 계층 및 하나의 출력 계층을 포함하는 노드 계층.
* 예를 들면 특성 Input 3개에 대하여 가중치와 바이어스를 다르게 한 후 특성 2개를 새로 뽑아 낸 후 2개의 특성에서 결과를 도출.
## 선형 변환
* 신경망에서 선형 변환을 하게 되면 두 가지의 선형 변환을 하나의 선형 변환으로 치환할 수 있다.
* 하나의 선형 변환으로 치환할 수 있기 때문에 Layer를 깊게 쌓아도 결국에 하나의 Layer와 같게 된다.
## 비 선형 변환
* 선형 변환의 문제점을 해결 함.
* 비 선형 변환은 두 가지의 비 선형 변환이 하나의 선형 변환으로 치환 될 수 없다.
* 활성화 함수를 사용한 비 선형 변환으로 Layer를 깊게 쌓을 수 있다.
* 대표적인 활성화 함수는 Sigmoid, ReLU
    * Sigmoid함수는 기울기 소실(Gradient vanishing) 문제가 있음.
    * ReLU함수는 이러한 문제를 해결하기 위해 제안됨.